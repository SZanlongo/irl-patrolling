{"name":"Irl-patrolling","tagline":"Generate patrolling paths via Inverse Reinforcement Learning","body":"# irl-patrolling\r\nGenerate patrolling paths via Inverse Reinforcement Learning\r\n\r\nModification of Sergey Levein's work, found at: <https://graphics.stanford.edu/projects/gpirl/>\r\n\r\nOriginal ReadMe below:\r\n\r\nSergey Levine, 2011\r\n\r\n### 1. Introduction\r\n\r\nThis MATLAB package contains a collection of inverse reinforcement learning\r\nalgorithms and a framework for evaluating these algorithms on a variety of\r\nsimple Markov decision processes. The package is distributed primarily as the\r\nreference implementation for the Gaussian Process Inverse Reinforcement Learning\r\nalgorithm described in the paper \"Nonlinear Inverse Reinforcement Learning with\r\nGaussian Processes\" (Levine, Popovic, Koltun, NIPS 2011), but is also intended\r\nto provide a general-purpose framework for researchers interested in inverse\r\nreinforcement learning. This file describes the contents of the package,\r\nprovides instructions regarding its use, instructions for extending the package\r\nwith new algorithms, solvers, and example domains, and finally contains the\r\nlicense under which the package may be used and distributed.\r\n\r\n### 2. Installation\r\n\r\nThe IRL toolkit was built with MATLAB R2009b. Earlier version of MATLAB may be\r\nsufficient, but were not tested. In addition to MATLAB R2009b and various\r\ntoolboxes, which may include the Statistics and Parallelism toolboxes, the\r\npackage requires the following external MATLAB scripts:\r\n\r\n- minFunc, by Mark Schmidt, included with this package (Creative Commons by-nc,\r\n    see <http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html> for details)\r\n- CVX, by Michael C. Grant and Stephen P. Boyd, included with this package (GPL)\r\n- plot2svg, by Juerg Schwizer, included with this package (FreeBSD)\r\n\r\nWith the exception of CVX, none of the scripts require installation.\r\nInstallation consists of two easy steps:\r\n\r\n\t1. Extract all files in irl_toolkit.zip into a desired directory.\r\n\t2. Run Utilities/cvx_setup to install CVX.\r\n\r\n### 3. Usage\r\n\r\nThis section briefly describes how to use the IRL toolkit. The toolkit is\r\ndesigned to be modular and extendable. Most sessions will begin by running\r\n\"addpaths\" in the main directory, to add all necessary subdirectories to the\r\npath. Optionally, you may save your MATLAB path to avoid needing to do this, but\r\nthis is left up to you, since some users may prefer not to clutter up their\r\npath.\r\n\r\n##### 3.1 Running a single test\r\n\r\nAn example script for running a test is provided under the name singletest.m in\r\nthe main directory. This script calls \"addpaths\" to add the necessary paths, and\r\nthen runs a single test using GPIRL on a 32x32 objectworld environment. The\r\nfunction \"runtest\" creates a random environment of a specified type, creates\r\nsynthetic examples (or, optionally, uses provided human generated examples), and\r\ncalls the specified IRL algorithm to determine the reward function. The result\r\nof the IRL algorithm is evaluated according to a set of provided metrics, and\r\nthe results are returned in a single structure. The arguments of runtest are the\r\nfollowing:\r\n\r\n\t1. Algorithm name. This always corresponds to the name of the directory\r\n        containing the algorithm, which is also the prefix of every function for that\r\n        algorithm.\r\n\t2. Algorithm parameters. This is a struct containing the desired parameters for\r\n        this algorithm. For more information, see <algorithm name>defaultparams.m in\r\n        the desired algorithm's directory.\r\n\t3. MDP type. This is the MDP solver used to generate synthetic examples. By\r\n        default, two options are available: standardmdp, which uses the standard\r\n        Bellman equations, and linearmdp, which uses the LMDP framework.\r\n\t4. Example name. This is the directory name/prefix of the environment to be\r\n        tested.\r\n\t5. Example parameters. These are the parameters of the test environment. See\r\n        <example name>defaultparams.m in the desired example's directory.\r\n\t6. Test parameters. These are parameters of the test itself, including the\r\n        number of examples. See General/setdefaultparams.m for details.\r\n\r\nA quick way to view the results of the test is provided by the printresults\r\nfunction. The visualize function renders the results of the test to the screen,\r\nwith a side by side comparison of the true and learned reward functions.\r\n\r\n##### 3.2 Running a transfer test\r\n\r\nTo run a transfer test -- that is, to test how well a learned reward function\r\ngeneralizes to another state space, -- use the runtransfertest function. The\r\narguments of this function are the following:\r\n\r\n\t1. IRL results returned by runtest. If runtest returns test_result, this is\r\n        test_result.irl_result\r\n\t2. Algorithm name, as before.\r\n\t3. MDP type.\r\n\t4. Example name. Should be the same as the example in the original test.\r\n\t5. Example parameters. These may be different from the original test (in\r\n        fact, they should be to test any interesting transfer). However, some\r\n        parameters may be incompatible -- for example, in the objectworld, the\r\n        number of features depends on \"n\", so testing transfer with a different\r\n        values of \"n\" will not work when using discrete features.\r\n\t6. Transfer test parameters. See General/settransferdefaultparams.m for\r\n        details.\r\n\r\nThe results of the transfer test can similarly be printed and viewed using\r\nprintresults and visualize.\r\n\r\n##### 3.3 Running a series of tests\r\n\r\nThe functions runtestseries and runtransferseries can be used to run a large\r\nseries of tests, using a variety of algorithms and parameter settings. The\r\npackage contains scripts for graphing and saving the results of such tests.\r\nExamples of how to use these functions can be found in the NIPS11_Tests\r\ndirectory, which contains the scripts that were used to produce the graphs in\r\nthe 2011 NIPS paper (note that, due to code changes since publication and\r\nrandomized initialization, the results produced by these scripts may differ\r\nslightly from those presented in the paper). All of these scripts use the\r\nParallelism toolbox to run multiple tests in parallel for efficiency, and call\r\nthe saveresults function at the end to save the results from the test into a\r\ndirectory with a name corresponding to the test and the current time and date.\r\nOnce this directory is created, the results can be graphed using the\r\nrenderresults function, which takes the directory name as the first argument.\r\nThe second argument specifies whether to only graph the results (1), or whether\r\nto also render the reward function of each test (0), which takes significantly\r\nlonger.\r\n\r\n##### 3.4 Using human generated examples\r\n\r\nThe directory Human_Demos contains some demonstrations of highway policies by a\r\nhuman user. To pass these examples to a test, set the \"true_examples\" field of\r\nthe test parameters struct (last argument to runtest) to\r\nhuman_result.example_samples after loading the desired demo. Note that the test\r\nshould be done using the same example parameters, which can be obtained from\r\nhuman_result.mdp_params.\r\n\r\nTo generate your own human demonstrations, use the tools in HumanControl.\r\nSpecifically, the runhumantrial function will bring up an interface for\r\ngenerating a human demonstration. This interface is currently \"turn based\",\r\nalthough a real time interface may be added in the future. The arguments for\r\nthis function are the following:\r\n\r\n\t1. Example name.\r\n\t2. Example parameters.\r\n\t3. Human trial parameters specifying the number and length of examples. See\r\n        HumanControl/humantrialdefaultparams.m\r\n\r\nThe function returns a human_result struct, which can be saved and used for\r\nfuture tests.\r\n\r\n### 4. Directory overview\r\n\r\nThis section presents an overview of the directories in the package, briefly\r\ndescribing each included IRL algorithm, MDP solver, and example domain.\r\n\r\n##### 4.1 IRL algorithms\r\n\r\nNote that, with the exception of FIRL and GPIRL, I cannot vouch for the accuracy\r\nof my implementations of prior algorithms. While I made a best faith effort to\r\nimplement these methods correctly based on the authors' descriptions, these are\r\nmy own implementations and should not be viewed as the definitive reference\r\nimplementations of the prior algorithms.\r\n\r\n\t- AN - Abbeel & Ng's projection algorithm for inverse reinforcement learning.\r\nNote that, since Abbeel & Ng's algorithm does not return a single reward, this\r\nimplementation is somewhat less accurate than the original algorithm, because a\r\nsingle reward function must be selected to return for evaluation. For details,\r\nsee \"Apprenticeship Learning via Inverse Reinforcement Learning\" (Abbeel and Ng,\r\nICML 2004) \r\n\t- FIRL - The Feature Construction for Inverse Reinforcement Learning algorithm.\r\nOnly compatible with discrete features (the \"continuous\" parameter must be set\r\nto 0 for the current example environment). For details, see \"Feature\r\nConstruction for Inverse Reinforcement Learning\" (Levine, Popovic, Koltun, NIPS\r\n2010)\r\n\t- GPIRL - The Gaussian Process Inverse Reinforcement Learning algorithm. For\r\ncontinuous features, it is recommended to use the warped kernel by setting the\r\n\"warp_x\" parameter to 1. For details, see \"Nonlinear Inverse Reinforcement\r\nLearning with Gaussian Processes\" (Levine, Popovic, Koltun, NIPS 2011)\r\n\t- LEARCH - An implement of the \"Learning to Search\" algorithm. This is a \"best\r\neffort\" implementation, since the algorithm is quite complicated and has many\r\nfree parameters. The current version can run either log-linear mode or\r\nnonlinear, with decision trees of logistic regression to create nonlinear\r\nfeatures. For details, see \"Learning to Search: Functional Gradient Techniques\r\nfor Imitation Learning\" (Ratliff, Silver, Bagnell, Autonomous Robots 27 (1)\r\n2009)\r\n\t- MaxEnt - the maximum entropy IRL algorithm. For details, see \"Maximum Entropy\r\nInverse Reinforcement Learning\" (Ziebart, Mass, Bagnell, Dey, AAAI 2008)\r\n\t- MMP - the maximum margin planning algorithm. Note that this implementation\r\nuses the QP formulation of MMP rather than subgradient methods, since the QP\r\nversion is already very fast on the examples in this package. For details, see\r\n\"Maximum Margin Planning\" (Ratliff, Bagnell, Zinkevich, ICML 2006)\r\n\t- MMPBoost - MMP with feature boosting, using decision trees of a configurable\r\ndepth. As with LEARCH, this is a \"best effort\" implementation, since the\r\nalgorithm has many free parameters. For details, see \"Boosting Structured\r\nPrediction for Imitation Learning\" (Ratliff, Bradley, Bagnell, Chestnutt, NIPS\r\n2007)\r\n\t- MWAL - The game-theoretic MWAL algorithm. For details, see \"A Game-Theoretic\r\nApproach to Apprenticeship Learning\" (Syed, Schapire, NIPS 2008)\r\n\t- OptV - A provisional implementation of the OptV algorithm. Since OptV learns\r\nvalue functions rather than reward functions, this algorithm cannot use the\r\nprovided features, and instead simply learns a value for each state. This makes\r\nit unable to perform transfer, and produces (unfairly) poor results compared to\r\nthe other methods. It should be noted that this is a symptom of the framework\r\nrather than the algorithm, since the toolkit is designed for testing algorithms\r\nthat learn rewards rather than value functions. For details, see \"Inverse\r\nOptimal Control with Linearly-Solvable MDPs\" (Dvijotham, Todorov, ICML 2010)\r\n\r\n##### 4.2 MDP solvers\r\n\r\nThe toolkit comes with two MDP solvers. The standard solver (StandardMDP) uses\r\nvalue iteration to recover a value function, which in turn specifies a\r\ndeterministic optimal policy for the MDP. The linear MDP solver (LinearMDP) uses\r\n\"soft\" value iteration as described in Ziebart's PhD thesis, and corresponds to\r\nlinearly-solvable MDPs (see Dvijotham & Todorov 2010). This model produces\r\nstochastic examples, which may be viewed as \"suboptimal\" under the standard MDP\r\nmodel.\r\n\r\n##### 4.3 Example domains\r\n\r\nThree example domains are included in this toolkit. They are described below:\r\n\r\n\t- Gridworld - This is an NxN gridworld, with 5 actions per state corresponding\r\nto moving in each direction and staying in place. The \"determinism\" parameter\r\nspecifies the probability that each action will \"succeed.\" If the action\r\n\"fails,\" a different random action is taken instead. The reward function of the\r\ngridworld consists of BxB blocks of cells (forming a \"super grid\"). The features\r\nare indicators for x and y values being below various integer values. Note that\r\nthis environment does not support transfer experiments.\r\n\t- Objectworld - This is a gridworld populated with objects. Each object has one\r\nof C inner and outer colors, and the objects are placed at random. There are 2C\r\ncontinuous features, each giving the Euclidean distance to the nearest object\r\nwith a specific inner or outer color. In the discrete feature case, there are\r\n2CN binary features, each one an indicator for a corresponding continuous\r\nfeature being less than some value D. The true reward is positive in states that\r\nare both within 3 cells of outer color 1 and 2 cells of outer color 2, negative\r\nwithin 3 cells of outer color 1, and zero otherwise. Inner colors and all other\r\nouter colors are distractors.\r\n\t- Highway - This is the highway environment described in \"Nonlinear Inverse\r\nReinforcement Learning with Gaussian Processes.\" Note that the number of cars on\r\nthe highway must be specified manually. There is a somewhat hacky algorithm for\r\nplacing cars that avoids creating roadblocks. This algorithm will stall if it\r\ncannot be placed the specified number of cars, so don't specify too many. This\r\nmay be fixed in a future release. It is also possible to specify more than 2\r\ncategories or classes of cars, though this functionality is currently untested.\r\n\r\n##### 4.4 General\r\n\r\nThe following are miscallaneous directories in the toolkit:\r\n\r\n\t- Evaluation - This directory contains implementations of the various metrics\r\nthat can be used to evaluate the IRL result. Each function is a particular\r\nmetric.\r\n\t- General - These are general testing scripts, including runtest,\r\nruntransfertest, etc.\r\n\t- HumanControl - Interface for generating human demonstrations.\r\n\t- NIPS11_Tests - Scripts for reproducing test results from \"Nonlinear Inverse\r\nReinforcement Learning with Gaussian Processes.\"\r\n\t- Testing - Scripts for graphing, saving, and visualizing test results from\r\nseries tests.\r\n\t- Utilities - External utilities, including minFunc and CVX.\r\n\r\n### 5. Calling conventions\r\n\r\nThe IRL toolkit is intended to be extendable and modular. This section describes\r\nthe calling convention for various modules, in order to describe how additional\r\ncomponents can be added. So long as the new component implements the specified\r\nfunctions, it will be usable with the toolkit.\r\n\r\n##### 5.1 IRL algorithms\r\n\r\nIRL algorithms should must implement two functions: <name>run and\r\n<name>transfer, where <name> is the name of the algorithm that will be used to\r\nidentify it when calling runtest. Additionally, all current algorithms implement\r\n<name>defaultparams, but this is a convenience. The run function must accept the\r\nfollowing arguments:\r\n\r\n\t1. algorithm_params - The parameters of the algorithm. Some parameters may be\r\nmissing, so it is advisable to implement a <name>defaultparams function to fill\r\nthem in (for example, see gpirldefaultparams.m).\r\n\t2. mdp_data - A specification of the example domain. mdp_data includes the\r\nnumber of states, the number of actions, the transition function (specified by\r\nsa_s and sa_p), and so forth. For details, see (for example) gridworldbuild.m\r\n\t3. mdp_model - The name of the current MDP model (standardmdp or linearmdp).\r\nThe final returned policy must be computed from the reward function using this\r\nmodel, but most algorithms will not use this parameter anywhere else.\r\n\t4. feature_data - Information about the features. The main paramater of this\r\nstruct is feature_data.splittable (forgive the \"legacy\" name), which is a matrix\r\ncontaining the value of each feature at each state.\r\n\t5. example_samples - A cell array of the examples, where exmaple_samples{i,t}\r\nis time step t along trajectory i. Each entry in the cell array has two numbers:\r\nexample_samples{i,t}(1) is the state, and example_samples{i}{t}(2) is the\r\naction.\r\n\t6. true_features - The true features that form a linear basis for the reward.\r\nMost algorithms (that are not cheating) will ignore this, but it may be useful\r\nfor establishing a baseline comparison.\r\n\t7. verbosity - The desired amount of output. 0 means no output, 1 means\r\n\"medium\" output, and 2 means \"verbose\" output.\r\n\r\nAdditionally, IRL algorithms must return the struct irl_result containing the\r\nresult of the computation. This struct has the following fields:\r\n\r\n\t1. r - the learned reward function, with mdp_data.states rows and\r\nmdp_data.actions columns\r\n\t2. v - the resulting value function, returned by <mdp_model>solve\r\n\t3. p - the corresponding policy (as above)\r\n\t4. q - the corresponding q function (as above)\r\n\t5. r_itr - the reward at each iteration of the algorithm, useful for debugging;\r\nif unused, set to {{r}}\r\n\t6. model_itr - the model at each iteration of the algorithm, to be used for\r\ntransfer\r\n\t7. model_r_itr - a second optional reward at each iteration of the algorithm,\r\nuseful for debugging; if unused, set to r_itr\r\n\t8. p_itr - the policy at each iteration; if unused, set to {{p}}\r\n\t9. model_p_itr - a second optional policy at each iteration of the algorithm,\r\nuseful for debugging; if unused, set to p_itr\r\n\t10. time - the time the algorithm spent computing the reward function\r\n(optional, set to 0 if unused)\r\n\r\nThe transfer function must use the learned model to transfer the reward function\r\nto a different state space. The output of this function is identical to\r\n<name>run. The arguments are the following:\r\n\r\n\t1. prev_result - The irl_result structure returned by a <name>run call.\r\n\t2. mdp_data - MDP definition for the new state space.\r\n\t3. mdp_model - MDP model, as before.\r\n\t4. feature_data - Feature data for the new state space.\r\n\t5. true_feature_map - True features for the new state space (again, not used\r\nexcept when \"cheating\").\r\n\t6. verbosity - As before.\r\n\r\nIf both <name>run and <name>transfer are implemented, the algorithm should be\r\nusable in all tests without further modification.\r\n\r\n##### 5.2 MDP solvers\r\n\r\nAdditional MDP solvers can also be added to the framework, for example for\r\nsolving problems with continuous state spaces. The MDP solver must implement 5\r\nfunctions: <name>solve, <name>frequency, <name>action, <name>step, and\r\n<name>compare.\r\n\r\nThe function <name>solve must find the solution to the specified MDP. It has two\r\narguments: mdp_data, the MDP definition, and the states x actions reward r. The\r\ndefinition mdp_data is constructed by the example building function (see 5.3).\r\nThe output is a struct \"mdp_solution\", which has fields \"v\", \"q\", and \"p\",\r\ncorresponding to the value function, Q function, and policy. Note that these\r\nfields will only be used by other functions relating to your MDP solver, so they\r\ncan contain whatever information you choose.\r\n\r\nThe function <name>frequency finds the visitation frequency of all states in the\r\nMDP. This function is used by many metrics. The inputs are mdp_data and\r\nmdp_solution (from a previous call to <name>solve), and the output is a column\r\nvector with 1 entry for each state, giving that state's expected visitation\r\ncount under the policy in mdp_solution.\r\n\r\nThe functions <name>action and <name>step sample an action at a particular state\r\nand execute that action, respectively. <name>action takes as input mdp_data,\r\nmdp_solution, and a state s. The output is an action a, which is either the\r\noptimal action under a deterministic policy or a sampled action under a\r\nstochastic policy. The function <name>step takes mdp_data, mdp_solution, a state\r\ns, and an action a, and returns the resulting state after taking action a in\r\nstate s, which may be sampled if the MDP is nondeterministic.\r\n\r\nFinally, the function <name>compare compares two policies, given as arguments p1\r\nand p2, and returns the amount of discrepancy between them. In the case of a\r\nstandard MDP, this is currently the number of states in which the policies\r\ndisagree. In the case of the linear MDP, this is the sum over all states of the\r\nprobability that p1 and p2 will take different actions. This function is only\r\nused when evaluating some metrics.\r\n\r\n##### 5.3 Example domains\r\n\r\nExample domains are only required to implement two functions: <name>build and\r\n<name>draw. The building function has a single argument - the mdp_params\r\nstructure passed to runtest that specifies the parameters of the desired\r\nexample. The outputs are the following:\r\n\r\n\t1. mdp_data - A structure containing the definition of the MDP. It must specify\r\nthe following fields:\r\n\t\t- states - the number of states\r\n\t\t- actions - the number of actions in each state\r\n\t\t- discount - the discount factor of the MDP (currently all examples are\r\ninfinite horizon discounted reward)\r\n\t\t- sa_s - 3D matrix of size states x actions x K, where K is any number. The\r\nentry sa_s(s,a,k) specifies the state to transition to when taking action a in\r\nstate s, and sampling destination k (see below)\r\n\t\t- sa_p - 3D matrix as above, where sa_p(s,a,k) gives the probability of\r\ntransitioning to sa_s(s,a,k) on action a in state s\r\n\t2. r - A states x actions matrix specifying the true reward function of the\r\nexample environment.\r\n\t3. feature_data - A struct containing two fields regarding the features of the\r\nexample:\r\n\t\t- splittable a states x features matrix containing the value of each feature\r\nat each state\r\n\t\t- stateadjacency - A sparse states x states matrix with \"1\" for each pair of\r\nstates that are \"adjacent\" (have an action to transition from one to the other);\r\nthis is currently only used by FIRL.\r\n\t4. true_feature_map - A (sparse) states x features matrix that gives the values\r\nof the \"true\" features that can be linearly combined to obtain r.\r\n\r\nThe drawing function has no output, and takes the following input arguments:\r\n\r\n\t1. r - The reward function to draw.\r\n\t2. p - The policy to draw (may be deterministic or stochastic).\r\n\t3. mdp_params - The mdp_params struct passed to <name>build.\r\n\t4. mdp_data - The mdp_data struct returned by <name>build.\r\n\t5. feature_data - The feature_data struct returned by <name>build.\r\n\t6. model - The MDP model (standardmdp or linearmdp).\r\n\r\nThe drawing function must draw some visualization of the specified example to\r\nthe current axes.\r\n\r\n### 6 License\r\n\r\nThe license included below governs the terms of use for this software. Please\r\ndirect any correspondence regarding the software to svlevine@cs.stanford.edu.\r\nThe IRL toolkit is created by Sergey Levine, copyright 2011. If you are using\r\nthe software in an academic publication, you may cite it as \"IRL Toolkit\" with a\r\nreference to the webpage from which it was obtained.\r\n\r\nCopyright (c) 2011, Sergey Levine\r\nAll rights reserved.\r\n\r\nThis software is made available under the Creative Commons \r\nAttribution-Noncommercial License, viewable at\r\nhttp://creativecommons.org/licenses/by-nc/3.0/. You are free to use, copy,\r\nmodify, and re-distribute the work.  However, you must attribute any\r\nre-distribution or adaptation in the manner specified below, and you may not\r\nuse this work for commercial purposes without the permission of the author.\r\n\r\nAny re-distribution or adaptation of this work must contain the author's name \r\n(Sergey Levine) and a link to the software's original webpage.\r\n\r\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \r\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \r\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \r\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \r\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \r\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \r\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \r\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \r\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \r\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \r\nPOSSIBILITY OF SUCH DAMAGE.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}